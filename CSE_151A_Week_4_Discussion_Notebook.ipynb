{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jaw039/Google-Colab-Notebooks/blob/main/Copy_of_CSE_151A_Week_4_Discussion_Notebook.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'numpy'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msklearn\u001b[39;00m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'numpy'"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import sklearn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ELHBSXhbR7QR"
      },
      "source": [
        "---\n",
        "## Welcome to the Week 4 Discussion!\n",
        "\n",
        "Hello everyone! Today, we're going to build on the lecture material and get you fully prepared for Homework 3 and SA4. We'll focus on implementing key algorithms from scratch to ensure you understand how they work under the hood.\n",
        "\n",
        "**Our agenda for today:**\n",
        "1.  K means clustering algorithm\n",
        "2.  Hierarchial Clustering\n",
        "3.  Singular Value Decomposition (SVD)\n",
        "4.  Principal Component Analysis (PCA)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "77bbd943"
      },
      "source": [
        "## Generate Dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "fcfac6af"
      },
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'sklearn'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[4], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdatasets\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m make_blobs\n\u001b[1;32m      3\u001b[0m X, y_true \u001b[38;5;241m=\u001b[39m make_blobs(n_samples\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m, centers\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m, n_features\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'sklearn'"
          ]
        }
      ],
      "source": [
        "from sklearn.datasets import make_blobs\n",
        "\n",
        "X, y_true = make_blobs(n_samples=100, centers=4, n_features=2, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 487
        },
        "id": "oyvCDB9xalO9",
        "outputId": "0ea2ca99-69ca-4852-9c8d-f8b67dc060d0"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(6, 5))\n",
        "plt.scatter(X[:, 0], X[:, 1], c='red')\n",
        "plt.title('Dataset')\n",
        "plt.xlabel('X1')\n",
        "plt.ylabel('X2')\n",
        "plt.grid(True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ho10J2q-S81F"
      },
      "source": [
        "## K-Means Clustering\n",
        "\n",
        "K-Means is an iterative partitioning clustering algorithm that aims to partition $n$ observations into $k$ clusters, where each observation belongs to the cluster with the nearest mean (centroid).\n",
        "\n",
        "[![k-means-clustering.png](https://i.postimg.cc/xdgfDNG6/k-means-clustering.png)](https://postimg.cc/Z9Bt6RMN)\n",
        "\n",
        "**Algorithm Steps:**\n",
        "\n",
        "1.  **Initialization**: Choose $k$ initial centroids. This is often done randomly from the data points.\n",
        "2.  **Assignment**: Assign each data point to the nearest centroid based on a distance metric (commonly Euclidean distance).\n",
        "    $$ \\text{cluster}_i = \\{ \\mathbf{x} \\mid \\| \\mathbf{x} - \\mathbf{c}_i \\|^2 \\le \\| \\mathbf{x} - \\mathbf{c}_j \\|^2 \\quad \\forall j \\ne i \\} $$\n",
        "    where $\\mathbf{x}$ is a data point, and $\\mathbf{c}_i$ and $\\mathbf{c}_j$ are centroids.\n",
        "3. **Update**: Recalculate the centroids for each cluster as the mean of all data points assigned to that cluster:\n",
        "$$\n",
        "\\mathbf{c}_i = \\frac{1}{|\\text{cluster}_i|} \\sum_{\\mathbf{x} \\in \\text{cluster}_i} \\mathbf{x}\n",
        "$$\n",
        "\n",
        "4.  **Convergence**: Repeat steps 2 and 3 until the centroids no longer change significantly or a maximum number of iterations is reached.\n",
        "\n",
        "**Objective Function:**\n",
        "\n",
        "K-Means aims to minimize the within-cluster sum of squares (WCSS), also known as inertia:\n",
        "$$ \\text{WCSS} = \\sum_{i=1}^{k} \\sum_{\\mathbf{x} \\in \\text{cluster}_i} \\| \\mathbf{x} - \\mathbf{c}_i \\|^2 $$\n",
        "\n",
        "It is the sum of squared distances from each data point to the center of its assigned cluster. It tells us how well the points are grouped in their clusters. The smaller the WCSS the better the clustering."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c09f4d5a"
      },
      "source": [
        "## K-means clustering implementation\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a062c90c",
        "outputId": "8ec990a5-40d1-49a1-eb2d-4aa2038d517f"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "def kmeans_from_scratch(X, n_clusters, max_iter=300, random_state=None):\n",
        "    \"\"\"\n",
        "    K-Means clustering from scratch using NumPy.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    X : (n_samples, n_features) ndarray\n",
        "        The input data.\n",
        "    n_clusters : int\n",
        "        The number of clusters. This is same as k.\n",
        "    max_iter : int\n",
        "        Maximum number of iterations for the K-Means algorithm.\n",
        "    random_state : int or np.random.Generator or None\n",
        "        Seed for random number generation.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    labels : (n_samples,) ndarray\n",
        "        Cluster labels for each sample.\n",
        "    centers : (n_clusters, n_features) ndarray\n",
        "        Cluster centers.\n",
        "    \"\"\"\n",
        "    rng = np.random.default_rng(random_state)\n",
        "    n_samples, n_features = X.shape\n",
        "\n",
        "    # 1. Initialize cluster centers\n",
        "    # Randomly pick n_clusters data points as initial centers\n",
        "    random_indices = rng.choice(n_samples, size=n_clusters, replace=False)\n",
        "    centers = X[random_indices]\n",
        "\n",
        "    for _ in range(max_iter):\n",
        "        # 2. Assign each data point to the nearest center\n",
        "        # Calculate distances between each data point and each center\n",
        "        distances = np.sqrt(((X - centers[:, np.newaxis])**2).sum(axis=2))\n",
        "        # Assign each point to the cluster with the minimum distance\n",
        "        labels = np.argmin(distances, axis=0)\n",
        "\n",
        "        # 3. Update cluster centers\n",
        "        new_centers = np.array([X[labels == i].mean(axis=0) if np.sum(labels == i) > 0 else centers[i] for i in range(n_clusters)])\n",
        "\n",
        "        # 4. Check for convergence\n",
        "        if np.allclose(centers, new_centers):\n",
        "            break\n",
        "        centers = new_centers\n",
        "\n",
        "    return labels, centers\n",
        "\n",
        "# Apply K-Means from scratch algorithm\n",
        "cluster_labels_scratch, cluster_centers_scratch = kmeans_from_scratch(X, n_clusters=4, random_state=42)\n",
        "\n",
        "print(\"Cluster Labels (first 10 points in the dataset):\", cluster_labels_scratch[:10])\n",
        "print(\"Cluster Centers:\\n\", cluster_centers_scratch)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 505
        },
        "id": "HFoZF25dZ_hu",
        "outputId": "2e2ec0f3-a7e5-42ef-9d6c-3b14e1ff1432"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Visualize K-Means results\n",
        "plt.figure(figsize=(12, 5))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.scatter(X[:, 0], X[:, 1], c=cluster_labels_scratch, s=50, cmap='viridis')\n",
        "plt.scatter(cluster_centers_scratch[:, 0], cluster_centers_scratch[:, 1], c='red', s=200, alpha=0.75, marker='X')\n",
        "plt.title('K-Means Clustering')\n",
        "plt.xlabel('Feature 1')\n",
        "plt.ylabel('Feature 2')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Qd_2UCUMOlO"
      },
      "source": [
        "### How to find the best value of k?\n",
        "The goal is to find the value of K where the decrease in the WCSS error starts to slow down. This is like an \"elbow\" in the plot where the curve bends and flattens out. The number of clusters at this point is considered the optimal value for K.\n",
        "\n",
        "[![elbow.png](https://media.geeksforgeeks.org/wp-content/uploads/elbow_3.jpeg)](https://media.geeksforgeeks.org/wp-content/uploads/elbow_3.jpeg)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uJrrgsiFTEQs"
      },
      "source": [
        "## Hierarchical Clustering\n",
        "Hierarchical clustering builds a hierarchy of clusters, either by merging small clusters into larger ones (agglomerative) or by splitting large clusters into smaller ones (divisive).\n",
        "\n",
        "Agglomerative (bottom-up): start with each point as its own cluster, and keep merging the closest ones.\n",
        "\n",
        "Divisive (top-down): start with all points in one cluster, and keep splitting them apart.\n",
        "\n",
        "We will focus on the more common agglomerative approach.\n",
        "\n",
        "**Agglomerative Algorithm Steps:**\n",
        "\n",
        "1.  **Initialization**: Start with each data point as its own cluster.\n",
        "2.  **Merging**: Iteratively merge the two closest clusters based on a chosen linkage criterion.\n",
        "3.  **Termination**: Continue merging until only one cluster remains or a desired number of clusters is reached.\n",
        "\n",
        "[![hierarchial-clustering.png](https://i.postimg.cc/9MWDyjbn/hierarchial-clustering.png)](https://postimg.cc/PLRfk9BW)\n",
        "\n",
        "**Distance Metrics:**\n",
        "\n",
        "The distance between data points is typically measured using metrics like Euclidean distance:\n",
        "$$ d(\\mathbf{x}_i, \\mathbf{x}_j) = \\| \\mathbf{x}_i - \\mathbf{x}_j \\|_2 $$\n",
        "\n",
        "**Linkage Criteria:**\n",
        "\n",
        "The linkage criterion determines how the distance between two clusters is calculated. Common criteria include:\n",
        "\n",
        "*   **Single Linkage**: The minimum distance between any two points in the two clusters.\n",
        "    $$ D(C_i, C_j) = \\min_{\\mathbf{x} \\in C_i, \\mathbf{y} \\in C_j} d(\\mathbf{x}, \\mathbf{y}) $$\n",
        "*   **Complete Linkage**: The maximum distance between any two points in the two clusters.\n",
        "    $$ D(C_i, C_j) = \\max_{\\mathbf{x} \\in C_i, \\mathbf{y} \\in C_j} d(\\mathbf{x}, \\mathbf{y}) $$\n",
        "*   **Average Linkage**: The average distance between all pairs of points in the two clusters.\n",
        "    $$ D(C_i, C_j) = \\frac{1}{|C_i||C_j|} \\sum_{\\mathbf{x} \\in C_i} \\sum_{\\mathbf{y} \\in C_j} d(\\mathbf{x}, \\mathbf{y}) $$\n",
        "*   **Ward's Method**: Minimizes the variance of the merged cluster. It calculates the increase in the total within-cluster sum of squares after merging.\n",
        "    $$ \\Delta(C_i, C_j) = \\text{WCSS}(C_i \\cup C_j) - (\\text{WCSS}(C_i) + \\text{WCSS}(C_j)) $$\n",
        "\n",
        "The result of hierarchical clustering is often visualized as a dendrogram, which shows the sequence of merges and the distances at which they occurred."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lUwxt4u0TJU8"
      },
      "source": [
        "##Hierarchical Clustering Implementation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mze68ECKFWGz",
        "outputId": "efeab308-abc2-4ed9-863a-5a746ce8cc3b"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "def euclidean_distance(p1, p2):\n",
        "    \"\"\"Calculates the Euclidean distance between two points.\"\"\"\n",
        "    return np.sqrt(np.sum((p1 - p2)**2))\n",
        "\n",
        "def calculate_cluster_distance(cluster1, cluster2, linkage_method='single'):\n",
        "    \"\"\"\n",
        "    Calculates the distance between two clusters based on the linkage method.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    cluster1, cluster2 : list of indices\n",
        "        Lists of indices of data points belonging to each cluster.\n",
        "    linkage_method : str\n",
        "        'single', 'complete', or 'average'.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    distance : float\n",
        "        The calculated distance between the clusters.\n",
        "    \"\"\"\n",
        "    if linkage_method == 'single':\n",
        "        return min(euclidean_distance(X[i], X[j]) for i in cluster1 for j in cluster2)\n",
        "    elif linkage_method == 'complete':\n",
        "        return max(euclidean_distance(X[i], X[j]) for i in cluster1 for j in cluster2)\n",
        "    elif linkage_method == 'average':\n",
        "        return np.mean([euclidean_distance(X[i], X[j]) for i in cluster1 for j in cluster2])\n",
        "    else:\n",
        "        raise ValueError(\"Invalid linkage method\")\n",
        "\n",
        "\n",
        "def hierarchical_clustering_from_scratch(X, n_clusters, linkage_method='single'):\n",
        "    \"\"\"\n",
        "    Agglomerative Hierarchical Clustering from scratch using NumPy.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    X : (n_samples, n_features) ndarray\n",
        "        The input data.\n",
        "    n_clusters : int\n",
        "        The desired number of clusters to stop at.\n",
        "    linkage_method : str\n",
        "        'single', 'complete', 'average'.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    labels : (n_samples,) ndarray\n",
        "        Cluster labels for each sample.\n",
        "    \"\"\"\n",
        "    n_samples = X.shape[0]\n",
        "    # Initially, each data point is its own cluster\n",
        "    clusters = [[i] for i in range(n_samples)]\n",
        "    print(f\"No. of clusters in first iteration: {len(clusters)}\")\n",
        "    print(clusters)\n",
        "\n",
        "    # Merge clusters until the desired number of clusters is reached\n",
        "    while len(clusters) > n_clusters:\n",
        "        min_distance = float('inf')\n",
        "        merge_indices = (-1, -1)\n",
        "\n",
        "        # Find the two closest clusters\n",
        "        for i in range(len(clusters)):\n",
        "            for j in range(i + 1, len(clusters)):\n",
        "                dist = calculate_cluster_distance(clusters[i], clusters[j], linkage_method)\n",
        "                if dist < min_distance:\n",
        "                    min_distance = dist\n",
        "                    merge_indices = (i, j)\n",
        "\n",
        "        # Merge the two closest clusters\n",
        "        idx1, idx2 = merge_indices\n",
        "        merged_cluster = clusters[idx1] + clusters[idx2]\n",
        "        clusters.pop(idx2) # Remove the second cluster first to avoid index issues\n",
        "        clusters.pop(idx1)\n",
        "        clusters.append(merged_cluster)\n",
        "\n",
        "    print(f\"\\nNo. of clusters after final iteration: {len(clusters)}\")\n",
        "    for i in clusters:\n",
        "      print(i)\n",
        "\n",
        "    # Assign labels based on the final clusters\n",
        "    labels = np.zeros(n_samples, dtype=int)\n",
        "    for i, cluster in enumerate(clusters):\n",
        "        for data_index in cluster:\n",
        "            labels[data_index] = i\n",
        "\n",
        "    return labels\n",
        "\n",
        "# Apply Hierarchical Clustering from scratch\n",
        "hierarchical_labels = hierarchical_clustering_from_scratch(X, n_clusters=4, linkage_method='single')\n",
        "\n",
        "print(\"\\nHierarchical Cluster Labels (first 10 data points):\", hierarchical_labels[:10])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 487
        },
        "id": "9b8e5bd1",
        "outputId": "976593dc-4bcf-4f24-c857-c1259f6989db"
      },
      "outputs": [],
      "source": [
        "# Visualize Hierarchical Clustering results\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.scatter(X[:, 0], X[:, 1], c=hierarchical_labels, s=50, cmap='viridis')\n",
        "plt.title('Hierarchical Clustering (4 clusters)')\n",
        "plt.xlabel('Feature 1')\n",
        "plt.ylabel('Feature 2')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ud98m-U2-Vq8"
      },
      "source": [
        "###TODO - Discussion Participation\n",
        "\n",
        "1. What do you think are advantages of hierarchical clustering over K-Means? What are disadvantages?\n",
        "\n",
        "2. Do you think K means is sensitive to outliers? Explain with reasoning."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WRZ5K2iHLhJQ"
      },
      "source": [
        "## Dimensionality Reduction\n",
        "\n",
        "#### Curse of Dimensionality\n",
        "Real world datasets way too many features in the dataset, hard to visualize them and maybe some of them are redundant or sparse. <br >\n",
        "The model would also have a hard time if its computational complexity scales with the number of features.\n",
        "\n",
        "### In a Nutshell\n",
        "**Find a low-dimensional representation that preserves the original information in high-dimensional space as much as possible.** <br >\n",
        "What do we mean by original information? <br >\n",
        "Global information like the variance(richness) in the dataset, the pairwise distance/similarity in the dataset. <br >\n",
        "\n",
        "\n",
        "\n",
        "PCA is a **linear** dimensionality reduction method that preserves **variances**. <br >\n",
        "PCA can use SVD to perform dimensionality reduction."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fcc025d3"
      },
      "source": [
        "## Singular Value Decomposition (SVD)\n",
        "\n",
        "Singular Value Decomposition (SVD) is a powerful matrix factorization technique that decomposes any matrix $A$ into three other matrices: $U$, $S$, and $V^T$.\n",
        "\n",
        "Mathematically, SVD is represented as:\n",
        "\n",
        "$$ A = U S V^T $$\n",
        "\n",
        "Where:\n",
        "- $A$ is the original matrix of shape $(m, n)$.\n",
        "- $U$ is an orthogonal matrix of shape $(m, r)$ whose columns are the left singular vectors of $A$.\n",
        "- $S$ is a diagonal matrix of shape $(r, r)$ containing the singular values of $A$ on the diagonal, in descending order. The singular values are the square roots of the eigenvalues of $A^T A$ (or $A A^T$).\n",
        "- $V^T$ is the transpose of an orthogonal matrix $V$ of shape $(r, n)$ whose rows are the right singular vectors of $A$. The columns of $V$ are the eigenvectors of $A^T A$.\n",
        "- Here, r = Rank(A) i.e. the number of nonzero singular values. It represents the effective dimensionality of the information in A.\n",
        "\n",
        "SVD has numerous applications, including dimensionality reduction, noise reduction, and recommender systems."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RD3_t2dFwIXC"
      },
      "source": [
        "## Algorithm\n",
        "\n",
        "Given **A** (m×n):\n",
        "\n",
        "1. Compute **M** = **A**ᵀ**A**.\n",
        "\n",
        "2. Solve the eigenproblem **Mvᵢ = λᵢvᵢ**. \\\n",
        "Order eigenvalues λ₁ ≥ λ₂ ≥ ⋯ ≥ 0 and take corresponding orthonormal eigenvectors **v**ᵢ. (These are the columns of **V**.)\n",
        "\n",
        "3. Set σᵢ = √λᵢ. Form **Σ** with σᵢ on the diagonal (same ordering).\n",
        "\n",
        "4. For each σᵢ > 0, compute **u**ᵢ = **(1/σᵢ)Avᵢ**. These are orthonormal automatically for distinct σᵢ; collect them as columns of **U**.\n",
        "\n",
        "5. If you need a full **U** and there are fewer than m nonzero singular values, extend {**u**ᵢ} to an orthonormal basis of ℝᵐ (Gram–Schmidt or eigen-decompose **AA**ᵀ).\n",
        "\n",
        "6. Check **A** = **U****Σ****V**ᵀ."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tfItCpU1iv67",
        "outputId": "77f62395-050f-4d7c-e3ae-e6b472ed827a"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "def svd_from_scratch(A, epsilon=1e-12):\n",
        "    \"\"\"\n",
        "    Computes the Singular Value Decomposition of a matrix A using NumPy from scratch.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    A : (m, n) array_like\n",
        "        The input matrix.\n",
        "    epsilon : float\n",
        "        A small value to handle numerical stability when dividing by singular values.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    U : (m, r) ndarray\n",
        "        The left singular vectors as columns (r = rank).\n",
        "    s : (r,) ndarray\n",
        "        The singular values in descending order.\n",
        "    Vt : (r, n) ndarray\n",
        "        The right singular vectors as rows (V transposed).\n",
        "    \"\"\"\n",
        "    m, n = A.shape\n",
        "\n",
        "    # 1) Form A^T A and eigen-decompose it\n",
        "    M = A.T @ A  # (n, n)\n",
        "    eigvals, eigvecs = np.linalg.eig(M)\n",
        "\n",
        "    # 2) Sort eigenvalues/vectors descending (so sigma1 >= sigma2)\n",
        "    idx = np.argsort(eigvals)[::-1]\n",
        "    eigvals = eigvals[idx]\n",
        "    V = eigvecs[:, idx]  # (n, n) - right singular vectors\n",
        "\n",
        "    # 3) Keep only positive eigenvalues (within numerical precision)\n",
        "    positive_mask = eigvals > epsilon\n",
        "    eigvals = eigvals[positive_mask]\n",
        "    V = V[:, positive_mask]  # (n, r) where r = number of positive eigenvalues\n",
        "\n",
        "    # 4) Singular values are sqrt(eigenvalues)\n",
        "    sigmas = np.sqrt(eigvals)  # (r,)\n",
        "    r = len(sigmas)\n",
        "\n",
        "    # 5) Left singular vectors: u_i = (A v_i) / sigma_i\n",
        "    U = np.zeros((m, r))\n",
        "    for i in range(r):\n",
        "        if sigmas[i] > epsilon:\n",
        "            U[:, i] = (A @ V[:, i]) / sigmas[i]\n",
        "\n",
        "    return U, sigmas, V.T  # Return V.T to match numpy's convention\n",
        "\n",
        "def round_matrix(x):\n",
        "    return np.round(x, 4)\n",
        "\n",
        "# Test the implementation\n",
        "A = np.array([[1, 2], [3, 4], [5, 6]], dtype=float)\n",
        "print(\"Original matrix A:\")\n",
        "print(A)\n",
        "print(\"Shape of A:\", A.shape)\n",
        "\n",
        "# Our implementation\n",
        "U, s, Vt = svd_from_scratch(A)\n",
        "print(f\"\\nOur SVD - U shape: {U.shape}, s shape: {s.shape}, Vt shape: {Vt.shape}\")\n",
        "\n",
        "# Reconstruct A\n",
        "r = len(s)\n",
        "Sigma_rect = np.zeros((r, r))\n",
        "np.fill_diagonal(Sigma_rect, s)\n",
        "recon = U @ Sigma_rect @ Vt\n",
        "\n",
        "print(\"\\nOur reconstruction:\")\n",
        "print(\"A_reconstructed:\\n\", round_matrix(recon))\n",
        "print(\"Singular values:\", round_matrix(s))\n",
        "print(\"U:\\n\", round_matrix(U))\n",
        "print(\"Vt:\\n\", round_matrix(Vt))\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"NumPy SVD for comparison:\")\n",
        "\n",
        "# NumPy implementation\n",
        "U_np, s_np, Vt_np = np.linalg.svd(A, full_matrices=False)\n",
        "recon_np = U_np @ np.diag(s_np) @ Vt_np\n",
        "\n",
        "print(f\"NumPy SVD - U shape: {U_np.shape}, s shape: {s_np.shape}, Vt shape: {Vt_np.shape}\")\n",
        "print(\"NumPy reconstruction:\\n\", round_matrix(recon_np))\n",
        "print(\"NumPy singular values:\", round_matrix(s_np))\n",
        "print(\"NumPy U:\\n\", round_matrix(U_np))\n",
        "print(\"NumPy Vt:\\n\", round_matrix(Vt_np))\n",
        "\n",
        "# Check if reconstructions match\n",
        "print(f\"\\nReconstruction error: {np.linalg.norm(recon - A):.2e}\")\n",
        "print(f\"Difference from NumPy: {np.linalg.norm(recon - recon_np):.2e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CwmGjoofvRU8"
      },
      "source": [
        "###TODO - Discussion Participation\n",
        "1. Why are singular values always nonnegative?\n",
        "\n",
        "2. What happens if rank 𝑟 < min(𝑚,𝑛)?  \n",
        "Hint: Think about what happens to the eigenvalues?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b46a8cd8"
      },
      "source": [
        "## Principal Component Analysis (PCA)\n",
        "\n",
        "It finds a linear mapping to project the data into low-dimensional space such that **the variances of the low-dimensional representation are maximized**.\n",
        "\n",
        "### How It Works From a High Level\n",
        "The low-dimensional space is spanned by **principal components** (they comprise the linear mapping). <br >\n",
        "\n",
        "Principal components are orthogonal(uncorrelated) to each other, so that they will not describe the same information. <br >\n",
        "\n",
        "Each principal component will try to align to the maximum variance in the dataset, by maximizing the distance between the projected data point and the origin (sum of squares).\n",
        "\n",
        "The principal components are the eigenvectors of the covariance matrix of the data, and the eigenvalues correspond to the variance explained by each component.\n",
        "\n",
        "[![pca.jpg](https://i.postimg.cc/3N82rvzW/pca.jpg)](https://postimg.cc/DJNSB8vK)\n",
        "\n",
        "\n",
        "**Algorithm Steps:**\n",
        "\n",
        "1.  **Standardize the Data**: Center the data by subtracting the mean of each feature. Optionally, scale the data by dividing by the standard deviation.\n",
        "    $$ X_{centered} = X - \\bar{X} $$\n",
        "2.  **Compute the Covariance Matrix**: Calculate the covariance matrix of the centered data.\n",
        "    $$ \\text{Cov}(X_{centered}) = \\frac{1}{n-1} X_{centered}^T X_{centered} $$\n",
        "3.  **Compute Eigenvalues and Eigenvectors**: Calculate the eigenvalues and eigenvectors of the covariance matrix.\n",
        "    $$ \\text{Cov}(X_{centered}) \\mathbf{w} = \\lambda \\mathbf{w} $$\n",
        "    where $\\lambda$ is an eigenvalue and $\\mathbf{w}$ is an eigenvector.\n",
        "4.  **Sort Eigenpairs**: Sort the eigenvalues in descending order and arrange the corresponding eigenvectors (principal components) accordingly.\n",
        "5.  **Select Principal Components**: Choose the top $k$ eigenvectors to form a projection matrix.\n",
        "6.  **Project Data**: Transform the original centered data onto the new lower-dimensional space defined by the selected principal components.\n",
        "    $$ Z = X_{centered} W $$\n",
        "    where $W$ is the matrix of the top $k$ eigenvectors.\n",
        "\n",
        "The explained variance ratio for each component indicates the proportion of the total variance in the data that is captured by that component.\n",
        "\n",
        "$$ \\text{Explained Variance Ratio}_i = \\frac{\\lambda_i}{\\sum_{j=1}^n \\lambda_j} $$\n",
        "\n",
        "**NOTE:** PCA is closely related to SVD. The columns of V (right singular vectors) are the principal components. So PCA can be done directly with SVD, without forming the covariance matrix."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OL1cuiy97Jax",
        "outputId": "ba56b6cf-3dfa-45ed-d378-3b9b9adc5f47"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "def pca_from_scratch(X, n_components, center=True):\n",
        "    \"\"\"\n",
        "    Performs Principal Component Analysis (PCA) on a dataset using NumPy from scratch.\n",
        "    \"\"\"\n",
        "    X = np.asarray(X, dtype=float)\n",
        "    n_samples, n_features = X.shape\n",
        "\n",
        "    # 1. Center the data\n",
        "    mean_ = X.mean(axis=0) if center else np.zeros(n_features)\n",
        "    Xc = X - mean_\n",
        "\n",
        "    # 2. Compute the covariance matrix\n",
        "    cov_matrix = (Xc.T @ Xc) / max(n_samples - 1, 1)\n",
        "\n",
        "    # 3. Eigen decomposition\n",
        "    eigenvalues, eigenvectors = np.linalg.eig(cov_matrix)\n",
        "\n",
        "    # 4. Sort by eigenvalues\n",
        "    sorted_indices = np.argsort(eigenvalues)[::-1]\n",
        "    lambdas = eigenvalues[sorted_indices]\n",
        "    W = eigenvectors[:, sorted_indices]  # eigenvectors as columns\n",
        "\n",
        "    # 5. Keep top components\n",
        "    lambdas = lambdas[:n_components]\n",
        "    W = W[:, :n_components]\n",
        "\n",
        "    # 6. Project the data\n",
        "    Z = Xc @ W\n",
        "\n",
        "    # Explained variance ratio\n",
        "    total_variance = np.sum(np.diag(eigenvalues))\n",
        "    explained_variance_ratio = lambdas / (total_variance + 1e-32)\n",
        "\n",
        "    return Z, W, explained_variance_ratio\n",
        "\n",
        "\n",
        "# ==== Example dataset (10 samples, 3 features -> project to 2D) ====\n",
        "X = np.array([\n",
        "    [2.5, 2.4, 1.2],\n",
        "    [0.5, 0.7, 1.8],\n",
        "    [2.2, 2.9, 0.9],\n",
        "    [1.9, 2.2, 1.5],\n",
        "    [3.1, 3.0, 2.1],\n",
        "    [2.3, 2.7, 1.4],\n",
        "    [2.0, 1.6, 0.8],\n",
        "    [1.0, 1.1, 1.0],\n",
        "    [1.5, 1.6, 1.7],\n",
        "    [1.1, 0.9, 0.6]\n",
        "])\n",
        "\n",
        "# Apply PCA (scratch implementation)\n",
        "Z, W, evr = pca_from_scratch(X, n_components=2, center=True)\n",
        "\n",
        "print(f\"Original Data:\\n\", X)\n",
        "print(\"\\n=================== Implementation from Scratch ====================\")\n",
        "print(\"\\nProjected data (Z):\\n\", Z)\n",
        "print(\"\\nPCA components (W):\\n\", W.T)\n",
        "print(\"\\nExplained variance ratio:\\n\", evr)\n",
        "\n",
        "print(\"\\n================ Sklearn implementation =========================\")\n",
        "pca = PCA(n_components=2)\n",
        "Z_sklearn = pca.fit_transform(X)\n",
        "print(\"\\nProjected data (Z):\\n\", Z_sklearn)\n",
        "print(\"\\nScikit-learn PCA components:\\n\", pca.components_)\n",
        "print(\"\\nExplained variance ratio:\\n\", pca.explained_variance_ratio_)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rA1pArLvLIGo"
      },
      "source": [
        "# Implement Calculation of Eigenvalues and Eigenvectors from scratch\n",
        "We use QR decomposition to find out the eigenvalues.\n",
        "\n",
        "The basic idea of QR decomposition is writing the matrix as a product of an orthogonal matrix and an upper triangular matrix, multiply the factors in the reverse order, and iterate. We keep on iterating till we can convert the input matrix into a diagonal matrix form.\n",
        "\n",
        "**NumPy equivalent - np.linalg.eig(matrix)**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9hgi2fm9xUJG",
        "outputId": "96be13b6-ca87-4f8c-cb1d-8c63ca9cec66"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "def eigen_qr(A, max_iter=1000, tol=1e-10):\n",
        "    \"\"\"\n",
        "    Compute eigenvalues and eigenvectors of a square matrix using the QR algorithm.\n",
        "\n",
        "    Args:\n",
        "        A (np.ndarray): Square matrix\n",
        "        max_iter (int): Maximum number of iterations\n",
        "        tol (float): Convergence tolerance\n",
        "\n",
        "    Returns:\n",
        "        eigenvalues (np.ndarray)\n",
        "        eigenvectors (np.ndarray)\n",
        "    \"\"\"\n",
        "    n = A.shape[0]\n",
        "    A = A.astype(float)\n",
        "\n",
        "    # Initialize the accumulator for the eigenvectors.\n",
        "    # After convergence, columns of Q_total approximate eigenvectors of the original A.\n",
        "    Q_total = np.eye(n)\n",
        "\n",
        "    for _ in range(max_iter):\n",
        "        # QR factorization: A = Q R with Q orthonormal and R upper-triangular.\n",
        "        # NumPy uses a stable method (Householder reflections) under the hood.\n",
        "        Q, R = np.linalg.qr(A)\n",
        "\n",
        "        # Similarity-like update: A_{k+1} = R @ Q\n",
        "        # Since A = Q R, we can show Q^T A Q = R Q (for orthonormal Q, Q^T = Q^{-1}).\n",
        "        # Repeatedly applying this tends to push A toward upper-triangular form.\n",
        "        A = R @ Q\n",
        "\n",
        "        # Accumulate the Q's to build eigenvectors:\n",
        "        # If A_k ≈ Q_total @ T @ Q_total^T with T ≈ diagonal, then\n",
        "        # A_original @ Q_total ≈ Q_total @ T, so columns of Q_total are eigenvectors.\n",
        "        Q_total = Q_total @ Q\n",
        "\n",
        "        # Convergence check:\n",
        "        # We measure \"how non-diagonal\" A is by zeroing out the diagonal and\n",
        "        # taking the Frobenius norm of the remaining entries. As the iteration\n",
        "        # converges, off-diagonal terms shrink toward 0.\n",
        "        off_diag_norm = np.linalg.norm(A - np.diag(np.diagonal(A)))\n",
        "\n",
        "        # If the off-diagonal is sufficiently small, we consider the matrix\n",
        "        # effectively triangular/diagonal and stop iterating.\n",
        "        if off_diag_norm < tol:\n",
        "            break\n",
        "\n",
        "    # After convergence (or hitting max_iter), the diagonal entries of A are\n",
        "    # our eigenvalue estimates. Note: no guaranteed sorting.\n",
        "    eigenvalues = np.diagonal(A)\n",
        "\n",
        "    # Columns of Q_total are the corresponding eigenvectors.\n",
        "    # They are already orthonormal because each Q was orthonormal.\n",
        "    eigenvectors = Q_total\n",
        "\n",
        "    return eigenvalues, eigenvectors\n",
        "\n",
        "\n",
        "# Example\n",
        "A = np.array([[4, 2, 1],\n",
        "              [0, 3, -1],\n",
        "              [0, 0, 2]], dtype=float)\n",
        "\n",
        "vals, vecs = eigen_qr(A)\n",
        "\n",
        "print(\"Input Matrix:\\n\", A)\n",
        "print(\"\\nEigenvalues:\\n\", vals)\n",
        "print(\"\\nEigenvectors (columns correspond to eigenvalues):\\n\", vecs)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "include_colab_link": true,
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3.10",
      "language": "python",
      "name": "py310"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
